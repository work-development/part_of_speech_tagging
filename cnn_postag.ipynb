{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjo4SvbJabJP"
   },
   "source": [
    "# Свёрточные нейросети и POS-теггинг\n",
    "\n",
    "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:42:57.976431Z",
     "start_time": "2019-10-29T19:42:57.959538Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zYHlhF9abJU",
    "outputId": "9c4d7b45-7b2a-41f0-f3f0-180fa4b6bf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyconll in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: spacy_udpipe in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy_udpipe) (3.6.1)\n",
      "Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy_udpipe) (1.3.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyconll\n",
    "!pip install spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:34.549739Z",
     "start_time": "2019-10-29T19:49:32.179692Z"
    },
    "id": "V1F9zVkOabJV"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5C1PRZiabJW"
   },
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0ZrFIgJmg-c",
    "outputId": "68cee243-3026-44d2-b471-d602e76f3ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-18 19:43:15--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14970950 (14M) [text/plain]\n",
      "Saving to: ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "./stepik-dl-nlp/dat 100%[===================>]  14.28M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2023-08-18 19:43:16 (207 MB/s) - ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’ saved [14970950/14970950]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "#!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://drive.google.com/file/d/1fsbrTM3UUzTy3Hz4KRcLXQRLDuJWXLnr/view?usp=sharing\n",
    "\n",
    "# Файл ru_syntagrus-ud-train.conllu скопировал вручную с компа в файл в нотбук\n",
    "!wget -O .datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:08.433599Z",
     "start_time": "2019-10-29T19:46:05.110693Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ_TNIBtabJW",
    "outputId": "65334e65-1f15-4896-e672-54165e2f70e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-18 19:43:17--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40736581 (39M) [text/plain]\n",
      "Saving to: ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "./stepik-dl-nlp/dat 100%[===================>]  38.85M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-08-18 19:43:19 (354 MB/s) - ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu’ saved [40736581/40736581]\n",
      "\n",
      "--2023-08-18 19:43:19--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14970950 (14M) [text/plain]\n",
      "Saving to: ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "./stepik-dl-nlp/dat 100%[===================>]  14.28M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-08-18 19:43:20 (349 MB/s) - ‘./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu’ saved [14970950/14970950]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.525561Z",
     "start_time": "2019-10-29T19:49:37.315213Z"
    },
    "id": "tW7otABbabJX"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.548127Z",
     "start_time": "2019-10-29T19:49:56.527559Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq1hxOhaabJY",
    "outputId": "714efc47-452f-40e4-b63c-0a480fd60f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.916262Z",
     "start_time": "2019-10-29T19:49:56.549806Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-Bw5oY-abJY",
    "outputId": "bb6647ca-24a4-4ba0-bdd3-d2263c459b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 194\n",
      "Наибольшая длина токена 31\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent if token.form)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:57.251433Z",
     "start_time": "2019-10-29T19:49:56.919818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIwkd_IuabJZ",
    "outputId": "dfd74951-03b4-4096-c34f-2eb81fd8914b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent if token.form) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.124148Z",
     "start_time": "2019-10-29T19:49:57.254191Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKq7dv87abJZ",
    "outputId": "b7ba9509-88a0-4348-d075-284c1cf034eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов 142\n",
      "[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n"
     ]
    }
   ],
   "source": [
    "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.524125Z",
     "start_time": "2019-10-29T19:49:58.125577Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HinZ6uAabJa",
    "outputId": "4347dbb1-8e05-47c2-95d8-09c122db3091"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<NOTAG>': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_ONLZhS3ZY5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from dlnlputils.pipeline import predict_with_model\n",
    "#from .base import tokenize_corpus\n",
    "\n",
    "\n",
    "def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n",
    "    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n",
    "    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n",
    "\n",
    "    for sent_i, sent in enumerate(sentences):\n",
    "        for token_i, token in enumerate(sent):\n",
    "            targets[sent_i, token_i] = label2id.get(token.upos, 0) # Если нет ключа token.upos в label2id то .get вернёт 0\n",
    "\n",
    "            if token.form:\n",
    "              for char_i, char in enumerate(token.form):\n",
    "                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.752672Z",
     "start_time": "2019-10-29T19:49:58.526431Z"
    },
    "id": "3bcbAOW6abJa"
   },
   "outputs": [],
   "source": [
    "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.754883Z",
     "start_time": "2019-10-29T19:49:40.582Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wI5v9H1abJa",
    "outputId": "056c9a69-dae2-42cc-c59c-546cc34a6ae1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 38,  4, 25,  4, 11, 19,  7,  6, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  2, 23, 11,  4,  9,  5,  7,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 17, 16, 10,  4, 12, 11,  3,  7,  6, 20,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  9, 12, 20, 21,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 40,  3, 15,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[1][:5] # Первые 5 слов предложения №2 (т.к индекс 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.756496Z",
     "start_time": "2019-10-29T19:49:40.711Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTfECrTgabJb",
    "outputId": "3a15a4cc-f72d-4830-aa62-595387270038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n",
       "         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1] # Для первого слова (train_inputs[1][0]) в предложении 2 мы должны предсказать класс 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi0WzAdOabJb"
   },
   "source": [
    "## Вспомогательная свёрточная архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOSpu9UgAFXV"
   },
   "source": [
    "nn.ModuleList — это просто список Python (хотя он полезен, поскольку параметры можно обнаружить и обучить с помощью оптимизатора). В то время как nn.Sequential — это модуль, который последовательно запускает компонент на входе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.316516Z",
     "start_time": "2019-10-29T19:46:17.539Z"
    },
    "id": "V0zXVO9tabJb"
   },
   "outputs": [],
   "source": [
    "# Реализация аналога простого ResNet\n",
    "# kernel_size=3 означает что в одном токене (слове) мы будем \"прощупывать\" (учитывать влияние контекста) 3-х рядом стоящих символов\n",
    "\n",
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2), # Сверточный слой принимает и возвращает одно и тоже число каналов; padding для неизменности размера тензора\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)   # Skip Connections\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEKbt5_1abJc"
   },
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2t4TLBVcwRJ",
    "outputId": "33f7915a-dad4-45d9-e247-5df2202d6f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Старый вид e:\n",
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]]])\n",
      "Старый размер: torch.Size([2, 2, 1])\n",
      "batch_size_n = 2, max_sent_len_n = 2\n",
      "Новый вид e:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "Новый размер e\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "e = torch.tensor([[[1],[2]],[[3],[4]]])\n",
    "print(f'Старый вид e:')\n",
    "print(e)\n",
    "print(f'Старый размер: {e.shape}')\n",
    "batch_size_n, max_sent_len_n, max_token_len_n =  e.shape\n",
    "print(f'batch_size_n = {batch_size_n}, max_sent_len_n = {max_sent_len_n}')\n",
    "e = e.view(batch_size_n * max_sent_len_n, max_token_len_n)\n",
    "print(f'Новый вид e:')\n",
    "print(e)\n",
    "print(f'Новый размер e')\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7RY8HJZda2x",
    "outputId": "ed851762-93c9-411d-e351-9d02386abd5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4339,  0.8487,  0.6920]],\n",
       "\n",
       "        [[-0.3160, -2.1152,  0.3223]],\n",
       "\n",
       "        [[-1.2633,  0.3500,  0.3081]],\n",
       "\n",
       "        [[ 0.1198,  1.2377, -0.1435]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_n = 10 # 10 тензоров\n",
    "embedding_size_n = 3\n",
    "ee = nn.Embedding(vocab_size_n, embedding_size_n, padding_idx=0)(e) # Создать 10 тензоров эмбедингов и выбрать из них тензора с номерами из списка \"e\"\n",
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odRjCPtyhute",
    "outputId": "69d22507-57b9-4d9f-b3af-5bb3541dadd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4339],\n",
       "         [ 0.8487],\n",
       "         [ 0.6920]],\n",
       "\n",
       "        [[-0.3160],\n",
       "         [-2.1152],\n",
       "         [ 0.3223]],\n",
       "\n",
       "        [[-1.2633],\n",
       "         [ 0.3500],\n",
       "         [ 0.3081]],\n",
       "\n",
       "        [[ 0.1198],\n",
       "         [ 1.2377],\n",
       "         [-0.1435]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee = ee.permute(0, 2, 1)\n",
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuPAM4IBjecl",
    "outputId": "cd9d07df-b2bf-433c-d4ad-3ba416a191ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU5Th2MRh6xD",
    "outputId": "b91e802a-bb01-4fcd-c0d1-b6ab81a8255a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0304],\n",
       "         [ 0.8485],\n",
       "         [ 1.0954]],\n",
       "\n",
       "        [[-0.3182],\n",
       "         [-1.5442],\n",
       "         [ 0.3206]],\n",
       "\n",
       "        [[-1.1456],\n",
       "         [ 0.4405],\n",
       "         [ 0.6901]],\n",
       "\n",
       "        [[ 0.6768],\n",
       "         [ 1.3504],\n",
       "         [ 0.4191]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_features = StackedConv1d(embedding_size_n)(ee)\n",
    "e_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    },
    "id": "_ELoPqZb8eX4"
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num) # labels_num --- количество меток частей речи\n",
    "        self.labels_num = labels_num\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len) # Забываем что токены были объеденены в предложения\n",
    "\n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize Для каждого символа получить вектора\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen (Размер батча=BatchSize*MaxSentenceLen, EmbSize-количество признаков для каждого элемента ) Транспонируем тензор чтобы его подать в свёрточную нейросеть\n",
    "\n",
    "        features = self.backbone(char_embeddings) # Содержит вектора символов уже с учетом контекста\n",
    "        # Теги нужно предсказывать не для каждого символа, а для каждого токена (слова) => нужно агрегировать признаки символов чтобы получить вектор токена\n",
    "\n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize Получаем один вектор, количество элементов в котором соответствует EmbSize, элемент = max(столбец фиксированного инд эмбединга для всех символов рассматриваемого слова)\n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum Вспоминаем что у нас есть предложения\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    },
    "id": "s-LqcZ6m8cmH"
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num) # labels_num --- количество меток частей речи\n",
    "        self.labels_num = labels_num\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len) # Забываем что токены были объеденены в предложения\n",
    "\n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize Для каждого символа получить вектора\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen (Размер батча=BatchSize*MaxSentenceLen, EmbSize-количество признаков для каждого элемента ) Транспонируем тензор чтобы его подать в свёрточную нейросеть\n",
    "\n",
    "        features = self.backbone(char_embeddings) # Содержит вектора символов уже с учетом контекста\n",
    "        # Теги нужно предсказывать не для каждого символа, а для каждого токена (слова) => нужно агрегировать признаки символов чтобы получить вектор токена\n",
    "\n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize Получаем один вектор, количество элементов в котором соответствует EmbSize, элемент = max(столбец фиксированного инд эмбединга для всех символов рассматриваемого слова)\n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum Вспоминаем что у нас есть предложения\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    },
    "id": "Zom_hsR5abJc"
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num) # labels_num --- количество меток частей речи\n",
    "        self.labels_num = labels_num\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len) # Забываем что токены были объеденены в предложения\n",
    "\n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize Для каждого символа получить вектора\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen (Размер батча=BatchSize*MaxSentenceLen, EmbSize-количество признаков для каждого элемента ) Транспонируем тензор чтобы его подать в свёрточную нейросеть\n",
    "\n",
    "        features = self.backbone(char_embeddings) # Содержит вектора символов уже с учетом контекста\n",
    "        # Теги нужно предсказывать не для каждого символа, а для каждого токена (слова) => нужно агрегировать признаки символов чтобы получить вектор токена\n",
    "\n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize Получаем один вектор, количество элементов в котором соответствует EmbSize, элемент = max(столбец фиксированного инд эмбединга для всех символов рассматриваемого слова)\n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum Вспоминаем что у нас есть предложения\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.318497Z",
     "start_time": "2019-10-29T19:46:23.764Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whXYnH9HabJc",
    "outputId": "fa66a6ae-b129-4a54-aa12-7803e9a7f7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 47314\n"
     ]
    }
   ],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.319470Z",
     "start_time": "2019-10-29T19:46:25.552Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nc4rEPVaabJc",
    "outputId": "3450b58b-7176-4b1a-9325-99901b17c680",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 384 итераций, 40.67 сек\n",
      "Среднее значение функции потерь на обучении 0.0962288748996798\n",
      "Среднее значение функции потерь на валидации 0.036881465225083995\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 384 итераций, 33.46 сек\n",
      "Среднее значение функции потерь на обучении 0.03174445396871306\n",
      "Среднее значение функции потерь на валидации 0.029106813344624963\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 384 итераций, 34.30 сек\n",
      "Среднее значение функции потерь на обучении 0.026835750628379174\n",
      "Среднее значение функции потерь на валидации 0.027410112653333363\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 384 итераций, 33.95 сек\n",
      "Среднее значение функции потерь на обучении 0.024526089716043014\n",
      "Среднее значение функции потерь на валидации 0.02460995739750048\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 384 итераций, 34.09 сек\n",
      "Среднее значение функции потерь на обучении 0.023234407031850424\n",
      "Среднее значение функции потерь на валидации 0.024650209122291294\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 384 итераций, 34.15 сек\n",
      "Среднее значение функции потерь на обучении 0.022008095125784166\n",
      "Среднее значение функции потерь на валидации 0.022690208223049003\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 384 итераций, 34.07 сек\n",
      "Среднее значение функции потерь на обучении 0.021235445274214726\n",
      "Среднее значение функции потерь на валидации 0.021322359460046385\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 384 итераций, 34.06 сек\n",
      "Среднее значение функции потерь на обучении 0.020776614309094537\n",
      "Среднее значение функции потерь на валидации 0.02119624786226466\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 384 итераций, 34.08 сек\n",
      "Среднее значение функции потерь на обучении 0.020412596020226676\n",
      "Среднее значение функции потерь на валидации 0.02055186062756151\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 384 итераций, 34.12 сек\n",
      "Среднее значение функции потерь на обучении 0.019962080000065423\n",
      "Среднее значение функции потерь на валидации 0.023315453846560846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_single_token_model) = train_eval_loop(single_token_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=10,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=500,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                       factor=0.5,\n",
    "                                                                                                                       verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.320568Z",
     "start_time": "2019-10-29T19:46:47.579Z"
    },
    "id": "krCTZKqmabJd"
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_single_token_model.state_dict(), './stepik-dl-nlp/models/single_token_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.321566Z",
     "start_time": "2019-10-29T19:46:47.731Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtibGkYgabJd",
    "outputId": "b8c34ac0-3240-45a1-8616-4f382615956b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "single_token_model.load_state_dict(torch.load('./stepik-dl-nlp/models/single_token_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.324276Z",
     "start_time": "2019-10-29T19:46:48.445Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYg3a5jrabJd",
    "outputId": "43f3a84d-7122-49fb-995d-9b183e44c621"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:12, 63.38it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.017917033284902573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.88      0.94      0.91     43357\n",
      "         ADP       1.00      0.99      0.99     39344\n",
      "         ADV       0.89      0.88      0.89     22733\n",
      "         AUX       0.86      0.72      0.79      3537\n",
      "       CCONJ       0.87      0.99      0.93     15168\n",
      "         DET       0.86      0.80      0.83     10781\n",
      "        INTJ       0.88      0.28      0.42        50\n",
      "        NOUN       0.97      0.94      0.96    103538\n",
      "         NUM       0.92      0.93      0.92      5640\n",
      "        PART       0.98      0.75      0.85     13556\n",
      "        PRON       0.89      0.86      0.88     18733\n",
      "       PROPN       0.87      0.91      0.89     14855\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.79      0.89      0.83      8057\n",
      "         SYM       1.00      0.99      0.99       420\n",
      "        VERB       0.92      0.96      0.94     47731\n",
      "           X       0.99      0.64      0.78       189\n",
      "\n",
      "    accuracy                           0.99   4756104\n",
      "   macro avg       0.92      0.86      0.88   4756104\n",
      "weighted avg       0.99      0.99      0.99   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275.0 [00:04<00:00, 64.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.024432189762592316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1549482\n",
      "         ADJ       0.83      0.93      0.88     14471\n",
      "         ADP       0.99      0.99      0.99     15062\n",
      "         ADV       0.84      0.87      0.86      8085\n",
      "         AUX       0.90      0.70      0.79      1518\n",
      "       CCONJ       0.88      0.99      0.94      5736\n",
      "         DET       0.85      0.77      0.81      4094\n",
      "        INTJ       0.83      0.22      0.34        23\n",
      "        NOUN       0.96      0.92      0.94     36568\n",
      "         NUM       0.91      0.90      0.91      2528\n",
      "        PART       0.98      0.73      0.84      4921\n",
      "        PRON       0.91      0.88      0.89      8015\n",
      "       PROPN       0.85      0.85      0.85      5883\n",
      "       PUNCT       1.00      1.00      1.00     29463\n",
      "       SCONJ       0.78      0.89      0.83      2992\n",
      "         SYM       1.00      1.00      1.00       165\n",
      "        VERB       0.91      0.95      0.93     18146\n",
      "           X       1.00      0.40      0.57        48\n",
      "\n",
      "    accuracy                           0.99   1707200\n",
      "   macro avg       0.91      0.83      0.85   1707200\n",
      "weighted avg       0.99      0.99      0.99   1707200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncY9SwRTabJd"
   },
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.325744Z",
     "start_time": "2019-10-29T19:46:50.139Z"
    },
    "id": "g5VEuhzQabJe"
   },
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "\n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "\n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.326925Z",
     "start_time": "2019-10-29T19:46:50.310Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RKJbhDhabJe",
    "outputId": "8e7ed3dc-0825-4d8f-9809-1c7794baa09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84370\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.327888Z",
     "start_time": "2019-10-29T19:46:50.737Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfNKGjNzabJe",
    "outputId": "1a99a50b-fb64-4fb3-8fc3-efe8ecacc85b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 384 итераций, 171.54 сек\n",
      "Среднее значение функции потерь на обучении 0.08028637797300082\n",
      "Среднее значение функции потерь на валидации 0.0306947062363719\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 384 итераций, 171.38 сек\n",
      "Среднее значение функции потерь на обучении 0.027914599670718115\n",
      "Среднее значение функции потерь на валидации 0.020162754793568414\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 384 итераций, 171.36 сек\n",
      "Среднее значение функции потерь на обучении 0.02283729436021531\n",
      "Среднее значение функции потерь на валидации 0.018671032742108448\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 384 итераций, 171.30 сек\n",
      "Среднее значение функции потерь на обучении 0.020233459166774992\n",
      "Среднее значение функции потерь на валидации 0.01696559874857269\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 384 итераций, 171.23 сек\n",
      "Среднее значение функции потерь на обучении 0.018788335835173104\n",
      "Среднее значение функции потерь на валидации 0.0164705785268014\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 384 итераций, 171.34 сек\n",
      "Среднее значение функции потерь на обучении 0.017635144960271038\n",
      "Среднее значение функции потерь на валидации 0.01491032118936724\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 384 итераций, 171.20 сек\n",
      "Среднее значение функции потерь на обучении 0.01705765977627986\n",
      "Среднее значение функции потерь на валидации 0.013247099137947997\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 384 итераций, 171.33 сек\n",
      "Среднее значение функции потерь на обучении 0.016366797523611847\n",
      "Среднее значение функции потерь на валидации 0.013573553662120116\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 384 итераций, 171.35 сек\n",
      "Среднее значение функции потерь на обучении 0.015973899637174327\n",
      "Среднее значение функции потерь на валидации 0.013340196346329285\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 384 итераций, 171.24 сек\n",
      "Среднее значение функции потерь на обучении 0.015652179092285223\n",
      "Среднее значение функции потерь на валидации 0.012564392571365185\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                              train_dataset,\n",
    "                                              test_dataset,\n",
    "                                              F.cross_entropy,\n",
    "                                              lr=5e-3,\n",
    "                                              epoch_n=10,\n",
    "                                              batch_size=64,\n",
    "                                              device='cuda',\n",
    "                                              early_stopping_patience=5,\n",
    "                                              max_batches_per_epoch_train=500,\n",
    "                                              max_batches_per_epoch_val=100,\n",
    "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                         factor=0.5,\n",
    "                                                                                                                         verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.542052Z",
     "start_time": "2019-08-29T13:56:16.529110Z"
    },
    "id": "WBbJPFu9abJe"
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), './stepik-dl-nlp/models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.564926Z",
     "start_time": "2019-08-29T13:56:16.544481Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQs2dSGLabJf",
    "outputId": "7e7e9d23-e42d-472c-9efe-aa9d69acb64f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "sentence_level_model.load_state_dict(torch.load('./stepik-dl-nlp/models/sentence_level_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.092139Z",
     "start_time": "2019-08-29T13:56:16.567242Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NskmeHSGabJf",
    "outputId": "9d7f2dbf-c128-4c97-ef57-2f7f3e61ee33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:11, 63.98it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.010789771564304829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.91      0.95      0.93     43357\n",
      "         ADP       1.00      0.99      0.99     39344\n",
      "         ADV       0.93      0.90      0.92     22733\n",
      "         AUX       0.91      0.87      0.89      3537\n",
      "       CCONJ       0.94      0.97      0.96     15168\n",
      "         DET       0.92      0.91      0.92     10781\n",
      "        INTJ       0.95      0.38      0.54        50\n",
      "        NOUN       0.98      0.96      0.97    103538\n",
      "         NUM       0.93      0.94      0.94      5640\n",
      "        PART       0.96      0.90      0.92     13556\n",
      "        PRON       0.95      0.91      0.93     18733\n",
      "       PROPN       0.95      0.97      0.96     14855\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.85      0.95      0.90      8057\n",
      "         SYM       1.00      0.99      0.99       420\n",
      "        VERB       0.95      0.96      0.95     47731\n",
      "           X       0.98      0.66      0.79       189\n",
      "\n",
      "    accuracy                           1.00   4756104\n",
      "   macro avg       0.95      0.90      0.92   4756104\n",
      "weighted avg       1.00      1.00      1.00   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275.0 [00:04<00:00, 62.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.015891045331954956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1549482\n",
      "         ADJ       0.87      0.94      0.90     14471\n",
      "         ADP       0.99      0.99      0.99     15062\n",
      "         ADV       0.90      0.89      0.89      8085\n",
      "         AUX       0.93      0.86      0.90      1518\n",
      "       CCONJ       0.95      0.96      0.96      5736\n",
      "         DET       0.92      0.86      0.89      4094\n",
      "        INTJ       1.00      0.26      0.41        23\n",
      "        NOUN       0.97      0.95      0.96     36568\n",
      "         NUM       0.93      0.92      0.93      2528\n",
      "        PART       0.94      0.86      0.90      4921\n",
      "        PRON       0.95      0.92      0.93      8015\n",
      "       PROPN       0.92      0.95      0.93      5883\n",
      "       PUNCT       1.00      1.00      1.00     29463\n",
      "       SCONJ       0.84      0.95      0.89      2992\n",
      "         SYM       1.00      1.00      1.00       165\n",
      "        VERB       0.95      0.95      0.95     18146\n",
      "           X       0.96      0.50      0.66        48\n",
      "\n",
      "    accuracy                           1.00   1707200\n",
      "   macro avg       0.94      0.88      0.89   1707200\n",
      "weighted avg       1.00      1.00      1.00   1707200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In19OaRcabJf"
   },
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.105418Z",
     "start_time": "2019-08-29T13:56:42.093744Z"
    },
    "id": "NV-imbKUabJf"
   },
   "outputs": [],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.125540Z",
     "start_time": "2019-08-29T13:56:42.106771Z"
    },
    "id": "wF8MJJeJabJf"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.148124Z",
     "start_time": "2019-08-29T13:56:42.126930Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiyweY4qabJf",
    "outputId": "247c2160-bf3b-4ccd-85b6-0ad94ef6c9d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 84.58it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-NOUN раму-NOUN\n",
      "\n",
      "косил-VERB косой-NOUN косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-ADJ будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX шесть-NUM\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.168810Z",
     "start_time": "2019-08-29T13:56:42.149698Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq81sz03abJg",
    "outputId": "0299dfc9-ab0b-49e3-e854-1ee251c8594e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 127.44it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-NOUN раму-NOUN\n",
      "\n",
      "косил-VERB косой-ADJ косой-ADJ косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-NOUN будланула-NOUN бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n",
      "\n",
      "ведро-NOUN дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX шесть-VERB\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr1Dh1e-abJg"
   },
   "source": [
    "## Свёрточный модуль своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.193140Z",
     "start_time": "2019-08-29T13:56:42.170233Z"
    },
    "id": "MghxVsQ5abJg"
   },
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "\n",
    "        batch_size, src_channels, sequence_len = x.shape\n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)\n",
    "            sequence_len = x.shape[-1]\n",
    "\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.210013Z",
     "start_time": "2019-08-29T13:56:42.194620Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbg4hl7KabJg",
    "outputId": "38de6b4e-ca66-44ef-b859-ad5e8f1e0c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84370\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:00.233326Z",
     "start_time": "2019-08-29T13:56:42.211456Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skZ2wI32abJg",
    "outputId": "568f90c0-0c76-4173-bf36-e230a37b24ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 384 итераций, 64.86 сек\n",
      "Среднее значение функции потерь на обучении 0.09295775835926179\n",
      "Среднее значение функции потерь на валидации 0.02613215414014193\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 384 итераций, 64.86 сек\n",
      "Среднее значение функции потерь на обучении 0.02407210282156787\n",
      "Среднее значение функции потерь на валидации 0.018735020052604745\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 384 итераций, 64.89 сек\n",
      "Среднее значение функции потерь на обучении 0.0203012618876528\n",
      "Среднее значение функции потерь на валидации 0.01792207546532154\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 384 итераций, 64.93 сек\n",
      "Среднее значение функции потерь на обучении 0.01852577231572165\n",
      "Среднее значение функции потерь на валидации 0.01673353584667686\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 384 итераций, 64.99 сек\n",
      "Среднее значение функции потерь на обучении 0.017565954628177376\n",
      "Среднее значение функции потерь на валидации 0.01547727872165713\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 384 итераций, 65.05 сек\n",
      "Среднее значение функции потерь на обучении 0.016717421686432015\n",
      "Среднее значение функции потерь на валидации 0.014288150391882599\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 384 итераций, 64.91 сек\n",
      "Среднее значение функции потерь на обучении 0.016332398299709894\n",
      "Среднее значение функции потерь на валидации 0.014100725983850437\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 384 итераций, 64.91 сек\n",
      "Среднее значение функции потерь на обучении 0.015957386208659347\n",
      "Среднее значение функции потерь на валидации 0.013744878679477048\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 384 итераций, 64.92 сек\n",
      "Среднее значение функции потерь на обучении 0.015443893122816613\n",
      "Среднее значение функции потерь на валидации 0.016426117592813944\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 384 итераций, 64.92 сек\n",
      "Среднее значение функции потерь на обучении 0.01518656676489627\n",
      "Среднее значение функции потерь на валидации 0.012387366315331494\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      F.cross_entropy,\n",
    "                                                      lr=5e-3,\n",
    "                                                      epoch_n=10,\n",
    "                                                      batch_size=64,\n",
    "                                                      device='cuda',\n",
    "                                                      early_stopping_patience=5,\n",
    "                                                      max_batches_per_epoch_train=500,\n",
    "                                                      max_batches_per_epoch_val=100,\n",
    "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                 factor=0.5,\n",
    "                                                                                                                                 verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:39.145214Z",
     "start_time": "2019-08-29T14:06:00.234936Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbkWX-yQabJh",
    "outputId": "102dd1e9-3175-49d2-97f1-f8253b1bbca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:19, 38.73it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.010462422855198383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.96      0.92      0.94     43357\n",
      "         ADP       1.00      0.99      0.99     39344\n",
      "         ADV       0.85      0.96      0.90     22733\n",
      "         AUX       0.89      0.90      0.90      3537\n",
      "       CCONJ       0.91      0.99      0.95     15168\n",
      "         DET       0.90      0.93      0.91     10781\n",
      "        INTJ       0.93      0.26      0.41        50\n",
      "        NOUN       0.97      0.97      0.97    103538\n",
      "         NUM       0.95      0.95      0.95      5640\n",
      "        PART       0.98      0.84      0.91     13556\n",
      "        PRON       0.96      0.91      0.93     18733\n",
      "       PROPN       0.96      0.95      0.95     14855\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.88      0.80      0.84      8057\n",
      "         SYM       1.00      0.99      0.99       420\n",
      "        VERB       0.94      0.98      0.96     47731\n",
      "           X       0.98      0.66      0.78       189\n",
      "\n",
      "    accuracy                           1.00   4756104\n",
      "   macro avg       0.95      0.89      0.90   4756104\n",
      "weighted avg       1.00      1.00      1.00   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275.0 [00:07<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.015999944880604744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1549482\n",
      "         ADJ       0.92      0.90      0.91     14471\n",
      "         ADP       1.00      0.99      0.99     15062\n",
      "         ADV       0.81      0.95      0.87      8085\n",
      "         AUX       0.92      0.91      0.91      1518\n",
      "       CCONJ       0.92      0.99      0.96      5736\n",
      "         DET       0.89      0.88      0.89      4094\n",
      "        INTJ       1.00      0.26      0.41        23\n",
      "        NOUN       0.96      0.96      0.96     36568\n",
      "         NUM       0.93      0.92      0.93      2528\n",
      "        PART       0.98      0.81      0.89      4921\n",
      "        PRON       0.96      0.92      0.94      8015\n",
      "       PROPN       0.95      0.91      0.93      5883\n",
      "       PUNCT       1.00      1.00      1.00     29463\n",
      "       SCONJ       0.87      0.79      0.83      2992\n",
      "         SYM       0.98      1.00      0.99       165\n",
      "        VERB       0.93      0.97      0.95     18146\n",
      "           X       0.96      0.54      0.69        48\n",
      "\n",
      "    accuracy                           1.00   1707200\n",
      "   macro avg       0.94      0.87      0.89   1707200\n",
      "weighted avg       1.00      1.00      1.00   1707200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59iakM7m9o0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dr1Dh1e-abJg"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
